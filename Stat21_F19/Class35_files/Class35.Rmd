---
title: "Class 35"
subtitle: "STAT 021"
author: "Suzanne Thornton"
institute: "Swarthmore College"
date: "2019/12/9 (updated: `r Sys.Date()`)"
output:
  xaringan::moon_reader:
 #   css: ["default", "assets/sydney-fonts.css", "assets/sydney.css"]
    self_contained: false # if true, fonts will be stored locally
#    includes:
 #     in_header: "assets/mathjax-equation-numbers.html"
    nature:
#      beforeInit: ["assets/remark-zoom.js", "https://platform.twitter.com/widgets.js"]
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: '16:9' # alternatives '16:9' or '4:3' or others e.g. 13:9
      navigation:
        scroll: false # disable slide transitions by scrolling
includes:
  in_header: mystyles.sty
---

```{r setup_pres, include=FALSE, echo=FALSE}
#devtools::install_github("ropenscilabs/icon")
#devtools::session_info('rmarkdown')

rm(list=ls())
library('tidyverse')
library('gridExtra')
library('broom')
library('cowplot')

library("RefManageR")
library("DT")

#setwd("~/Drive/Swat docs/Stat 21/Class9_files")
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(fig.path='Figs/',echo=TRUE, warning=FALSE, message=FALSE)

```

```{css, echo=FALSE}
pre {
  background: #FFBB33;
  max-width: 100%;
  overflow-x: scroll;
}

.scroll-output {
  height: 75%;
  overflow-y: scroll;
}

.scroll-small {
  height: 50%;
  overflow-y: scroll;
}
   
.red{color: #ce151e;}
.green{color: #26b421;}
.blue{color: #426EF0;}
```

 
```{r, comic32, echo=FALSE, fig.align='center', out.height=600}
knitr::include_graphics("Figs/data_sci_map.png")
```


---
## Logistic regression 

Logistic regression is basically the same as multiple linear regression with **one difference**: the response variable in logistic regression is binary. 


Thus the assumption we need for inference is no longer that $\espilon$ is Normally distributed but instead is that $\epsilon$ follows a Bernoulli distribution.


Everything else however (e.g. the interpretations of residual plots, the influence of strong multicollinearity, etc) are all the same as with MLR! 


Logistic regression works by using what's called the *logit function*. 


---
## Logistic regression 
### Logit function 

Suppose $p \in (0,1)$ is some probability and some binary random variable, $Y$, takes on the value $Y=1$ with probability $p$ and takes the value $Y=0$ with probability $(1-p)$. 


The **odds ratio** (i.e. the ratio of the chance of a success divided by the chance of a failure) is defined as 
$$OR = \frac{p}{1-p}.$$


The logit function is simply the logarithm of the odds ratio:
$$logit(p) = log\left(\frac{p}{1-p} \right).$$


---
## Logistic regression 
### Visualising the Odds Ratio and Logit functions 

```{r logitplot35, echo=FALSE, warnings=FALSE, fig.align='center', out.height=450, out.width=700}
set.seed(1)
p <- seq(0.001,0.999,length=500)
or <- p/(1-p)
lgit <- log(or)

plot1 <- ggplot() + geom_point(aes(x=p, y=or)) + ylim(0,10) + 
          xlab("p") + ylab("Odds Ratio")
plot2 <- ggplot() + geom_point(aes(x=p, y=lgit)) +
          xlab("p") + ylab("Logit")

grid.arrange(plot1,plot2, nrow=1)
```


---
## Logistic regression 

Rather than model the binary response, $Y$, as a linear function of some predictor variables, logistic regression models the probability that $Y=1$ as a linear function of some predictor variables. 

If $Y \sim \text{Bernoulli}(p)$, we know that $E[Y]= 1(p) + 0(1-p) = p$. 


So with $p=Pr[Y=1 \mid x_1, \dots, x_p]$, logistic regression fits the model
$$log\left( \frac{p}{1-p}\right)= \beta_0 + \beta_1 x_1 + \cdots + \beta_p x_p$$
or, equivalently, 
$$p = \frac{1}{1+e^{-\left(\beta_0 + \beta_1 x_1 + \cdots + \beta_p x_p\right)}}.$$

---
## Generalized linear models 

Logistic regression is a special case of what is called *generalized linear regression methods*. 

Rather than fit the equation of a line (or plane in multiple dimensions)
$$Y \mid x_1, \dots, x_p = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_p x_p + \epsilon,$$
generalized linear regression models fit
$$Y \mid x_1, \dots, x_p = f(x_1, x_2, \dots, x_p) + \epsilon$$
where $f$ can potentially be any other function of the predictor variables. 


---
## Logistic regression in R 
### Titanic data 

```{r}
?glm

# logit_mod <- glm( response ~ predictors, family=binomial(link='logit'), data = mydata)
# summary(logit_mod)
```




  