---
title: "Adjusted R-squared for MLR"
author: "STAT 021 with Prof Suzy"
institute: "Swarthmore College"
output:
  xaringan::moon_reader:
 #   css: ["default", "assets/sydney-fonts.css", "assets/sydney.css"]
    self_contained: false # if true, fonts will be stored locally
#    includes:
 #     in_header: "assets/mathjax-equation-numbers.html"
    nature:
#      beforeInit: ["assets/remark-zoom.js", "https://platform.twitter.com/widgets.js"]
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: '16:9' # alternatives '16:9' or '4:3' or others e.g. 13:9
      navigation:
        scroll: false # disable slide transitions by scrolling
includes:
  in_header: mystyles.sty
---

```{r setup_pres, include=FALSE, echo=FALSE}
#devtools::install_github("ropenscilabs/icon")
#devtools::session_info('rmarkdown')
rm(list=ls())
library('tidyverse')
library('gridExtra')
library('broom')
library('cowplot')
library("RefManageR")
library("DT")

options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(fig.path='Figs/',echo=TRUE, warning=FALSE, message=FALSE)
```

```{css, echo=FALSE}
pre {
  background: #FFBB33;
  max-width: 100%;
  overflow-x: scroll;
}

.scroll-output {
  height: 70%;
  overflow-y: scroll;
}

.scroll-small {
  height: 50%;
  overflow-y: scroll;
}
   
.red{color: #ce151e;}
.green{color: #26b421;}
.blue{color: #426EF0;}
```


## The effect of adding more predictor variables in a MLR  
### SAT model with .blue[two] predictors

```{r echo=FALSE}
SAT_data <- read_table2("Data/sat_data.txt", col_names=FALSE, cols(col_character(), col_double(), col_double(), col_double(), col_double(), col_double(), col_double(), col_double()))
colnames(SAT_data) = c("State", "PerPupilSpending", "StuTeachRatio", "Salary", "PropnStu", "SAT_verbal", "SAT_math", "SAT_tot")
SAT_data <- SAT_data %>% mutate(prop_taking_SAT = PropnStu/100) %>% select(-PropnStu)
```

.scroll-output[
```{r}
MLR_SAT2 <- lm(SAT_tot ~ PerPupilSpending + 
                         StuTeachRatio, data = SAT_data)
MLR_SAT2_sum <- summary(MLR_SAT2)
MLR_SAT2_sum
```
]
---
## SAT model with .blue[two] predictors

```{r echo=TRUE, eval=FALSE}
residual_plot_data2 <- SAT_data %>% 
                      mutate(residuals = MLR_SAT2_sum$residuals,fitted_vals = MLR_SAT2$fitted.values) %>%
                      select(-c(State, SAT_verbal, SAT_math))

ggplot(residual_plot_data2) +
  geom_point(aes(x=fitted_vals, y=residuals)) + 
  labs(title="Residual plot", subtitle="Public school SAT scores", x="Predicted SAT Scores", y="Residuals") + 
  geom_hline(yintercept=0)
```
---
## SAT model with .blue[two] predictors

```{r class20_model2, echo=FALSE}
residual_plot_data2 <- SAT_data %>% 
                      mutate(residuals = MLR_SAT2_sum$residuals,fitted_vals = MLR_SAT2$fitted.values) %>%
                      select(-c(State, SAT_verbal, SAT_math))

ggplot(residual_plot_data2) +
  geom_point(aes(x=fitted_vals, y=residuals)) + 
  labs(title="Residual plot for public school SAT scores", 
       subtitle = "Predictors: Per pupil spending, Student-teacher ratio",
       x="Predicted SAT Scores", y="Residuals") + 
  geom_hline(yintercept=0)
```

---
## SAT model with .blue[three] predictors
.scroll-output[
```{r echo=FALSE}
MLR_SAT3 <- lm(SAT_tot ~ PerPupilSpending +
                         StuTeachRatio + 
                         Salary, data = SAT_data)
MLR_SAT3_sum <- summary(MLR_SAT3)
MLR_SAT3_sum
```
]
---
## SAT model with .blue[three] predictors

```{r echo=FALSE, fig.align='center'}
residual_plot_data3 <- SAT_data %>% 
                      mutate(residuals = MLR_SAT3_sum$residuals, fitted_vals = MLR_SAT3$fitted.values) %>%
                      select(-c(State, SAT_verbal, SAT_math))

ggplot(residual_plot_data3) +
  geom_point(aes(x=fitted_vals, y=residuals)) + 
  labs(title="Residual plot for public school SAT scores", 
       subtitle = "Predictors: Per pupil spending, Student-teacher ratio, Teacher salary",
       x="Predicted SAT Scores", y="Residuals") + 
  geom_hline(yintercept=0)
```

---
## SAT model with .blue[three] predictors
### What did we notice? 

- As we increase the number of predictor variables, the R-squared value increases;

- Our estimate for $\sigma$ decreases; 

- The shape of the residuals plot changes;

- The significance of any given predictor variable can change;

- The overall F test changes as well. 


**Q:** Did anything stay the same? 
  

--
**A:** Not really. Either model choice here is different, which means that these models are what statisticians call *non-robust*.


---
## SAT model with .blue[four] predictors

.scroll-output[ 
```{r echo=FALSE}
MLR_SAT4 <- lm(SAT_tot ~ PerPupilSpending +
                         StuTeachRatio + 
                         Salary + 
                         prop_taking_SAT, data = SAT_data)
MLR_SAT4_sum <- summary(MLR_SAT4)
MLR_SAT4_sum
```
]
---
## SAT model with .blue[four] predictors

```{r echo=FALSE, fig.align='center'}
residual_plot_data4 <- SAT_data %>% 
                      mutate(residuals = MLR_SAT4_sum$residuals, fitted_vals = MLR_SAT4$fitted.values) %>%
                      select(-c(State, SAT_verbal, SAT_math))

ggplot(residual_plot_data4) +
  geom_point(aes(x=fitted_vals, y=residuals)) + 
  labs(title="Residual plot for public school SAT scores", 
       subtitle = "Predictors: Per pupil spending, Student-teacher ratio, Teacher salary, \n Proportion of students taking the SAT",
       x="Predicted SAT Scores", y="Residuals") + 
  geom_hline(yintercept=0)
```

---
## SAT model with .blue[four] predictors
### What did we notice? 

Again we see the same patterns as before. But note that our interpretation of the individual effects of each variable changes as well! 

---
##R squared
### Why does adding predictor variables increase $R^2$? 

$$R^2 = \frac{SS_{reg}}{SS_{tot}} = 1 - \frac{SS_{res}}{SS_{tot}}$$ 

What is $SS_{res}$? 


--
Recall $Var(\epsilon) = \sigma^2$ and $\hat{\sigma}^2 = \frac{SS_{res}}{n-2}.$


The way we determine the coefficients of our linear model (i.e. the $\hat{\beta}$'s) is by minimizing $SS_{res}$. Mathematically, minimizing $SS_{res}$ is equivalent to maximizing $R^2$. 


So this question can be rephrased as: .red[why does adding more predictor variables decrease] $SS_{res}$?

---
##R squared
### Why does adding more predictor variables decrease $SS_{res}$?

Because we define the regression estimates as the "least squares" estimates. That is, by definition, (in SLR for example)

$$\hat{\beta_0} = \bar{y} + \hat{\beta_1} \bar{x} \\
\hat{\beta_1} = \frac{\sum_{i=1}^{n}(x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^{n}(x_i - \bar{x})^2}$$

are the values that minimize $SS_{res}$, the squared distance between the data and the line. So as we have to estimate more and more $\hat{\beta}$'s, we continue to minimize $SS_{res}$, but we are able to make $SS_{res}$ even smaller than before. The main reason why is the logical fact stated below. 

--
  > If set $S$ has $n$ elements in it, we can find the minimum and the maximum of this set. If we add $t>0$ more elements to the set, it is possible that both the minimum and maximum values are more extreme than they were before. 


---
## R squared
### R - the programming language - output 

In the linear regression output of R we have 

- multiple R squared is the R squared that we define as $R^2 = \frac{SS_{reg}}{SS_{tot}}$; 
  
- adjusted R squared this imposes a penalty on the multiple R squared value that accounts for adding more predictor variables to the model.